#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å°è¯´TXTå¯¼å…¥å·¥å…·
æ”¯æŒé€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šç›®å½•å’Œåˆ†ç±»IDæ¥æ‰¹é‡å¯¼å…¥TXTå°è¯´
"""

import os
import re
import sys
import argparse
import pymysql
import chardet
import json
from datetime import datetime
from pathlib import Path

# æ•°æ®åº“é…ç½®
DB_CONFIG = {
    'host': 'X.X.X.X',
    'port': 3306,
    'user': 'novol',
    'password': 'XXXXXXXXXXXXXX',
    'database': 'novol',
    'charset': 'utf8mb4'
}

# å…¨å±€å¤±è´¥æ–‡ä»¶è·Ÿè¸ª
FAILED_FILES = {
    'encoding_errors': [],  # ç¼–ç é”™è¯¯çš„æ–‡ä»¶
    'content_errors': [],   # å†…å®¹é”™è¯¯çš„æ–‡ä»¶
    'database_errors': [],  # æ•°æ®åº“é”™è¯¯çš„æ–‡ä»¶
    'duplicate_files': [],  # é‡å¤çš„æ–‡ä»¶
    'other_errors': []      # å…¶ä»–é”™è¯¯çš„æ–‡ä»¶
}

def get_database_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        return pymysql.connect(**DB_CONFIG)
    except Exception as e:
        print(f"âŒ æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None

def verify_category_exists(category_id):
    """éªŒè¯åˆ†ç±»æ˜¯å¦å­˜åœ¨"""
    connection = get_database_connection()
    if not connection:
        return False, None
    
    try:
        with connection.cursor() as cursor:
            cursor.execute("SELECT id, name, description FROM categories WHERE id = %s", (category_id,))
            result = cursor.fetchone()
            if result:
                return True, {"id": result[0], "name": result[1], "description": result[2]}
            else:
                return False, None
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢åˆ†ç±»å¤±è´¥: {e}")
        return False, None
    finally:
        connection.close()

def list_available_categories():
    """æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„åˆ†ç±»"""
    connection = get_database_connection()
    if not connection:
        return
    
    try:
        with connection.cursor() as cursor:
            cursor.execute("SELECT id, name, description FROM categories ORDER BY id")
            categories = cursor.fetchall()
            
            print("\nğŸ“š å¯ç”¨çš„å°è¯´åˆ†ç±»:")
            print("-" * 50)
            print(f"{'ID':<4} {'åˆ†ç±»åç§°':<15} {'æè¿°'}")
            print("-" * 50)
            for cat in categories:
                desc = cat[2] if cat[2] else "æ— æè¿°"
                print(f"{cat[0]:<4} {cat[1]:<15} {desc}")
            print("-" * 50)
            
    except Exception as e:
        print(f"âŒ è·å–åˆ†ç±»åˆ—è¡¨å¤±è´¥: {e}")
    finally:
        connection.close()

def detect_file_encoding(file_path):
    """
    å¢å¼ºçš„æ–‡ä»¶ç¼–ç æ£€æµ‹
    è¿”å›: (encoding, confidence, content) æˆ– (None, 0, None)
    """
    try:
        # è¯»å–æ–‡ä»¶çš„ä¸€éƒ¨åˆ†è¿›è¡Œç¼–ç æ£€æµ‹
        with open(file_path, 'rb') as f:
            raw_data = f.read(10240)  # è¯»å–å‰10KBè¿›è¡Œæ£€æµ‹
        
        # ä½¿ç”¨chardetæ£€æµ‹ç¼–ç 
        detected = chardet.detect(raw_data)
        encoding = detected.get('encoding')
        confidence = detected.get('confidence', 0)
        
        print(f"   chardetæ£€æµ‹: {encoding} (ç½®ä¿¡åº¦: {confidence:.2f})")
        
        # å¦‚æœç½®ä¿¡åº¦å¤ªä½ï¼Œå°è¯•å¸¸è§ç¼–ç 
        if confidence < 0.7:
            print(f"   ç½®ä¿¡åº¦è¾ƒä½ï¼Œå°è¯•å¸¸è§ç¼–ç ...")
            
        # æŒ‰ä¼˜å…ˆçº§å°è¯•ç¼–ç 
        encoding_list = []
        
        # å¦‚æœchardetæ£€æµ‹çš„ç¼–ç ç½®ä¿¡åº¦é«˜ï¼Œä¼˜å…ˆä½¿ç”¨
        if encoding and confidence > 0.7:
            encoding_list.append(encoding)
        
        # æ·»åŠ å¸¸è§çš„ä¸­æ–‡ç¼–ç 
        common_encodings = [
            'utf-8', 'utf-8-sig',  # UTF-8 (with/without BOM)
            'gbk', 'gb2312', 'gb18030',  # ä¸­æ–‡ç¼–ç 
            'utf-16', 'utf-16le', 'utf-16be',  # UTF-16
            'big5',  # ç¹ä½“ä¸­æ–‡
            'ascii', 'latin1', 'cp1252'  # å…¶ä»–ç¼–ç 
        ]
        
        # å»é‡å¹¶ä¿æŒé¡ºåº
        for enc in common_encodings:
            if enc not in encoding_list:
                encoding_list.append(enc)
        
        # å°è¯•æ¯ç§ç¼–ç 
        for try_encoding in encoding_list:
            try:
                with open(file_path, 'r', encoding=try_encoding) as f:
                    content = f.read()
                
                # éªŒè¯å†…å®¹è´¨é‡
                if validate_content_quality(content, try_encoding):
                    print(f"   âœ… æˆåŠŸä½¿ç”¨ç¼–ç : {try_encoding}")
                    return try_encoding, confidence, content
                else:
                    print(f"   âŒ ç¼–ç  {try_encoding} å†…å®¹è´¨é‡å·®")
                    
            except (UnicodeDecodeError, UnicodeError) as e:
                print(f"   âŒ ç¼–ç  {try_encoding} å¤±è´¥: {str(e)[:50]}")
                continue
            except Exception as e:
                print(f"   âŒ ç¼–ç  {try_encoding} å¼‚å¸¸: {str(e)[:50]}")
                continue
        
        print(f"   âŒ æ‰€æœ‰ç¼–ç å°è¯•éƒ½å¤±è´¥")
        return None, 0, None
        
    except Exception as e:
        print(f"   âŒ ç¼–ç æ£€æµ‹å¼‚å¸¸: {e}")
        return None, 0, None

def validate_content_quality(content, encoding):
    """
    éªŒè¯è§£ç åçš„å†…å®¹è´¨é‡
    è¿”å›: True å¦‚æœå†…å®¹è´¨é‡å¥½ï¼ŒFalse å¦‚æœè´¨é‡å·®
    """
    if not content or len(content.strip()) < 10:
        return False
    
    # è®¡ç®—å¯æ‰“å°å­—ç¬¦æ¯”ä¾‹
    printable_chars = sum(1 for c in content if c.isprintable() or c.isspace())
    total_chars = len(content)
    printable_ratio = printable_chars / total_chars if total_chars > 0 else 0
    
    # è®¡ç®—ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹
    chinese_chars = sum(1 for c in content if '\u4e00' <= c <= '\u9fff')
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¿‡å¤šçš„æ›¿æ¢å­—ç¬¦
    replacement_chars = content.count('\ufffd')
    replacement_ratio = replacement_chars / total_chars if total_chars > 0 else 0
    
    print(f"     å¯æ‰“å°å­—ç¬¦æ¯”ä¾‹: {printable_ratio:.3f}")
    print(f"     ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹: {chinese_ratio:.3f}")
    print(f"     æ›¿æ¢å­—ç¬¦æ¯”ä¾‹: {replacement_ratio:.3f}")
    
    # è´¨é‡åˆ¤æ–­æ ‡å‡†
    if replacement_ratio > 0.01:  # æ›¿æ¢å­—ç¬¦è¶…è¿‡1%
        return False
    
    if printable_ratio < 0.8:  # å¯æ‰“å°å­—ç¬¦å°‘äº80%
        return False
    
    # å¯¹äºä¸­æ–‡æ–‡æ¡£ï¼Œä¸­æ–‡å­—ç¬¦åº”è¯¥å ä¸€å®šæ¯”ä¾‹
    if chinese_ratio > 0.1 and printable_ratio > 0.9:
        return True
    
    # å¯¹äºå…¶ä»–æ–‡æ¡£ï¼Œå¯æ‰“å°å­—ç¬¦æ¯”ä¾‹é«˜å³å¯
    if printable_ratio > 0.95:
        return True
    
    return False

def save_failed_files_report(output_dir="."):
    """ä¿å­˜å¤±è´¥æ–‡ä»¶æŠ¥å‘Š"""
    if not any(FAILED_FILES.values()):
        print("ğŸ“Š æ²¡æœ‰å¤±è´¥çš„æ–‡ä»¶ï¼Œæ— éœ€ç”ŸæˆæŠ¥å‘Š")
        return None
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    report_file = os.path.join(output_dir, f"import_failed_files_{timestamp}.json")
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_failed = sum(len(files) for files in FAILED_FILES.values())
    
    report_data = {
        "ç”Ÿæˆæ—¶é—´": datetime.now().isoformat(),
        "å¤±è´¥æ–‡ä»¶æ€»æ•°": total_failed,
        "å¤±è´¥åˆ†ç±»ç»Ÿè®¡": {
            "ç¼–ç é”™è¯¯": len(FAILED_FILES['encoding_errors']),
            "å†…å®¹é”™è¯¯": len(FAILED_FILES['content_errors']),
            "æ•°æ®åº“é”™è¯¯": len(FAILED_FILES['database_errors']),
            "é‡å¤æ–‡ä»¶": len(FAILED_FILES['duplicate_files']),
            "å…¶ä»–é”™è¯¯": len(FAILED_FILES['other_errors'])
        },
        "å¤±è´¥æ–‡ä»¶è¯¦æƒ…": FAILED_FILES
    }
    
    try:
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“Š å¤±è´¥æ–‡ä»¶æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
        print(f"   æ€»å¤±è´¥æ–‡ä»¶æ•°: {total_failed}")
        
        # æ˜¾ç¤ºå„ç±»é”™è¯¯ç»Ÿè®¡
        for error_type, count in report_data["å¤±è´¥åˆ†ç±»ç»Ÿè®¡"].items():
            if count > 0:
                print(f"   {error_type}: {count}")
        
        return report_file
        
    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥æ–‡ä»¶æŠ¥å‘Šå¤±è´¥: {e}")
        return None

def retry_failed_files(report_file, category_id, retry_type=None):
    """
    é‡è¯•å¤±è´¥çš„æ–‡ä»¶
    retry_type: 'encoding', 'content', 'database', 'duplicate', 'other' æˆ– None (å…¨éƒ¨é‡è¯•)
    """
    if not os.path.exists(report_file):
        print(f"âŒ æŠ¥å‘Šæ–‡ä»¶ä¸å­˜åœ¨: {report_file}")
        return False
    
    try:
        with open(report_file, 'r', encoding='utf-8') as f:
            report_data = json.load(f)
        
        failed_files = report_data.get("å¤±è´¥æ–‡ä»¶è¯¦æƒ…", {})
        
        # ç¡®å®šè¦é‡è¯•çš„æ–‡ä»¶
        files_to_retry = []
        if retry_type:
            retry_key = f"{retry_type}_errors"
            if retry_key in failed_files:
                files_to_retry = failed_files[retry_key]
                print(f"ğŸ”„ é‡è¯• {retry_type} é”™è¯¯æ–‡ä»¶: {len(files_to_retry)} ä¸ª")
            else:
                print(f"âŒ æ— æ•ˆçš„é‡è¯•ç±»å‹: {retry_type}")
                return False
        else:
            # é‡è¯•æ‰€æœ‰å¤±è´¥æ–‡ä»¶ï¼ˆé™¤äº†é‡å¤æ–‡ä»¶ï¼‰
            for key, files in failed_files.items():
                if key != 'duplicate_files':  # è·³è¿‡é‡å¤æ–‡ä»¶
                    files_to_retry.extend(files)
            print(f"ğŸ”„ é‡è¯•æ‰€æœ‰å¤±è´¥æ–‡ä»¶: {len(files_to_retry)} ä¸ª")
        
        if not files_to_retry:
            print("ğŸ“ æ²¡æœ‰éœ€è¦é‡è¯•çš„æ–‡ä»¶")
            return True
        
        # æ¸…ç©ºå¤±è´¥æ–‡ä»¶è®°å½•ä»¥è®°å½•æ–°çš„å¤±è´¥
        global FAILED_FILES
        FAILED_FILES = {
            'encoding_errors': [],
            'content_errors': [],
            'database_errors': [],
            'duplicate_files': [],
            'other_errors': []
        }
        
        print("-" * 60)
        
        success_count = 0
        for i, file_info in enumerate(files_to_retry, 1):
            file_path = file_info if isinstance(file_info, str) else file_info.get('file_path', file_info)
            print(f"[{i}/{len(files_to_retry)}] é‡è¯•: {os.path.basename(file_path)}")
            
            if import_txt_file(file_path, category_id):
                success_count += 1
            print()
        
        print("=" * 60)
        print(f"ğŸ”„ é‡è¯•å®Œæˆç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸ: {success_count}")
        print(f"   âŒ ä»ç„¶å¤±è´¥: {len(files_to_retry) - success_count}")
        
        # ä¿å­˜æ–°çš„å¤±è´¥æ–‡ä»¶æŠ¥å‘Š
        if any(FAILED_FILES.values()):
            new_report = save_failed_files_report()
            if new_report:
                print(f"ğŸ“Š æ–°çš„å¤±è´¥æ–‡ä»¶æŠ¥å‘Š: {new_report}")
        
        return success_count > 0
        
    except Exception as e:
        print(f"âŒ é‡è¯•å¤±è´¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return False

def clean_filename_basic(filename):
    """åŸºç¡€æ–‡ä»¶åæ¸…ç†ï¼Œæå–æ ‡é¢˜"""
    # ç§»é™¤æ‰©å±•å
    title = os.path.splitext(filename)[0]
    
    # ç§»é™¤å¸¸è§çš„æ ‡è®°
    patterns_to_remove = [
        r'\[æœä¹¦å§\]',
        r'-soushu.*',
        r'\.txt$',
        r'-\[æœä¹¦å§ç½‘å€\]',
        r'--.*',
    ]
    
    for pattern in patterns_to_remove:
        title = re.sub(pattern, '', title, flags=re.IGNORECASE)
    
    # æ¸…ç†å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    title = re.sub(r'\s+', ' ', title).strip()
    
    return title if title else filename

def extract_chapters_from_content(content):
    """ä»å†…å®¹ä¸­æå–ç« èŠ‚"""
    # å¸¸è§çš„ç« èŠ‚æ ‡é¢˜æ¨¡å¼
    chapter_patterns = [
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+ç« [^\n]*',
        r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡\d]+èŠ‚[^\n]*',
        r'Chapter\s*\d+[^\n]*',
        r'^\d+[\.\-\s]*[^\n]*',
        r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[\.\-ã€\s]*[^\n]*'
    ]
    
    lines = content.split('\n')
    chapters = []
    current_chapter = {"title": "ç¬¬ä¸€ç« ", "content": ""}
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
        is_chapter_title = False
        for pattern in chapter_patterns:
            if re.match(pattern, line) and len(line) < 200:
                # ä¿å­˜å½“å‰ç« èŠ‚
                if current_chapter["content"].strip():
                    chapters.append(current_chapter)
                
                # å¼€å§‹æ–°ç« èŠ‚
                current_chapter = {
                    "title": line,
                    "content": ""
                }
                is_chapter_title = True
                break
        
        if not is_chapter_title:
            current_chapter["content"] += line + "\n"
    
    # æ·»åŠ æœ€åä¸€ç« 
    if current_chapter["content"].strip():
        chapters.append(current_chapter)
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç« èŠ‚åˆ†å‰²ï¼Œå°†æ•´ä¸ªæ–‡ä»¶ä½œä¸ºä¸€ç« 
    if not chapters:
        chapters = [{"title": "æ­£æ–‡", "content": content}]
    
    return chapters

def import_txt_file(file_path, category_id, default_author="æœªçŸ¥ä½œè€…"):
    """å¯¼å…¥å•ä¸ªtxtæ–‡ä»¶"""
    filename = os.path.basename(file_path)
    title = clean_filename_basic(filename)
    
    print(f"ğŸ“– æ­£åœ¨å¯¼å…¥: {filename}")
    print(f"   æå–æ ‡é¢˜: {title}")
    print(f"   æ–‡ä»¶å¤§å°: {os.path.getsize(file_path) / 1024:.1f} KB")
    
    try:
        # ä½¿ç”¨å¢å¼ºçš„ç¼–ç æ£€æµ‹
        encoding, confidence, content = detect_file_encoding(file_path)
        
        if not content:
            error_info = {
                "file_path": file_path,
                "filename": filename,
                "title": title,
                "error": "æ— æ³•æ£€æµ‹æˆ–è¯»å–æ–‡ä»¶ç¼–ç ",
                "file_size": os.path.getsize(file_path),
                "timestamp": datetime.now().isoformat()
            }
            FAILED_FILES['encoding_errors'].append(error_info)
            print(f"âŒ ç¼–ç æ£€æµ‹å¤±è´¥: {filename}")
            return False
        
        if len(content.strip()) < 50:
            error_info = {
                "file_path": file_path,
                "filename": filename,
                "title": title,
                "error": "å†…å®¹è¿‡çŸ­",
                "content_length": len(content.strip()),
                "encoding": encoding,
                "timestamp": datetime.now().isoformat()
            }
            FAILED_FILES['content_errors'].append(error_info)
            print(f"âš ï¸  å†…å®¹è¿‡çŸ­ï¼Œè·³è¿‡: {title}")
            return False
        
        # è¿æ¥æ•°æ®åº“
        connection = get_database_connection()
        if not connection:
            error_info = {
                "file_path": file_path,
                "filename": filename,
                "title": title,
                "error": "æ•°æ®åº“è¿æ¥å¤±è´¥",
                "encoding": encoding,
                "timestamp": datetime.now().isoformat()
            }
            FAILED_FILES['database_errors'].append(error_info)
            return False
        
        try:
            with connection.cursor() as cursor:
                # æ£€æŸ¥å°è¯´æ˜¯å¦å·²å­˜åœ¨
                cursor.execute("SELECT id FROM novels WHERE title = %s", (title,))
                if cursor.fetchone():
                    error_info = {
                        "file_path": file_path,
                        "filename": filename,
                        "title": title,
                        "error": "å°è¯´æ ‡é¢˜å·²å­˜åœ¨",
                        "encoding": encoding,
                        "timestamp": datetime.now().isoformat()
                    }
                    FAILED_FILES['duplicate_files'].append(error_info)
                    print(f"âš ï¸  å°è¯´å·²å­˜åœ¨ï¼Œè·³è¿‡: {title}")
                    return False
                
                # æå–ç« èŠ‚
                chapters_data = extract_chapters_from_content(content)
                print(f"   æ£€æµ‹åˆ°ç« èŠ‚æ•°: {len(chapters_data)}")
                
                if not chapters_data:
                    error_info = {
                        "file_path": file_path,
                        "filename": filename,
                        "title": title,
                        "error": "æ— æ³•æå–ç« èŠ‚å†…å®¹",
                        "encoding": encoding,
                        "content_length": len(content),
                        "timestamp": datetime.now().isoformat()
                    }
                    FAILED_FILES['content_errors'].append(error_info)
                    print(f"âŒ æ— æ³•æå–ç« èŠ‚: {title}")
                    return False
                
                # æ’å…¥å°è¯´è®°å½•
                cursor.execute("""
                    INSERT INTO novels (title, author, description, category_id, total_chapters, word_count, status, created_at, updated_at)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    title,
                    default_author,
                    f"ä»txtæ–‡ä»¶å¯¼å…¥ï¼š{filename}ï¼ˆç¼–ç ï¼š{encoding}ï¼‰",
                    category_id,
                    len(chapters_data),
                    len(content),
                    'completed',
                    datetime.now(),
                    datetime.now()
                ))
                
                novel_id = cursor.lastrowid
                
                # æ’å…¥ç« èŠ‚
                for i, chapter_data in enumerate(chapters_data, 1):
                    cursor.execute("""
                        INSERT INTO chapters (novel_id, chapter_number, title, content, word_count, created_at)
                        VALUES (%s, %s, %s, %s, %s, %s)
                    """, (
                        novel_id,
                        i,
                        chapter_data["title"][:200],  # é™åˆ¶æ ‡é¢˜é•¿åº¦
                        chapter_data["content"],
                        len(chapter_data["content"]),
                        datetime.now()
                    ))
                
                connection.commit()
                print(f"âœ… æˆåŠŸå¯¼å…¥: {title}")
                print(f"   ç« èŠ‚æ•°: {len(chapters_data)}")
                print(f"   æ€»å­—æ•°: {len(content):,}")
                print(f"   ä½¿ç”¨ç¼–ç : {encoding}")
                print(f"   ç½®ä¿¡åº¦: {confidence:.2f}")
                return True
                
        except Exception as e:
            error_info = {
                "file_path": file_path,
                "filename": filename,
                "title": title,
                "error": f"æ•°æ®åº“æ“ä½œå¤±è´¥: {str(e)}",
                "encoding": encoding,
                "timestamp": datetime.now().isoformat()
            }
            FAILED_FILES['database_errors'].append(error_info)
            print(f"âŒ æ•°æ®åº“æ“ä½œå¤±è´¥ {title}: {e}")
            connection.rollback()
            return False
        finally:
            connection.close()
            
    except Exception as e:
        error_info = {
            "file_path": file_path,
            "filename": filename,
            "title": title,
            "error": f"æœªçŸ¥é”™è¯¯: {str(e)}",
            "file_size": os.path.getsize(file_path) if os.path.exists(file_path) else 0,
            "timestamp": datetime.now().isoformat()
        }
        FAILED_FILES['other_errors'].append(error_info)
        print(f"âŒ å¯¼å…¥å¤±è´¥ {title}: {e}")
        return False

def batch_import_directory(directory_path, category_id, max_files=None):
    """æ‰¹é‡å¯¼å…¥ç›®å½•ä¸‹çš„txtæ–‡ä»¶"""
    if not os.path.exists(directory_path):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        return False
    
    if not os.path.isdir(directory_path):
        print(f"âŒ è·¯å¾„ä¸æ˜¯ç›®å½•: {directory_path}")
        return False
    
    print(f"ğŸ“‚ æ‰«æç›®å½•: {directory_path}")
    
    # éªŒè¯åˆ†ç±»å­˜åœ¨
    category_exists, category_info = verify_category_exists(category_id)
    if not category_exists:
        print(f"âŒ åˆ†ç±»ID {category_id} ä¸å­˜åœ¨")
        list_available_categories()
        return False
    
    print(f"ğŸ“‚ ç›®æ ‡åˆ†ç±»: {category_info['name']} (ID: {category_id})")
    
    # è·å–txtæ–‡ä»¶åˆ—è¡¨
    txt_files = []
    for file in os.listdir(directory_path):
        if file.lower().endswith('.txt'):
            file_path = os.path.join(directory_path, file)
            if os.path.isfile(file_path):  # ç¡®ä¿æ˜¯æ–‡ä»¶è€Œä¸æ˜¯ç›®å½•
                txt_files.append(file_path)
    
    if not txt_files:
        print(f"âŒ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ°txtæ–‡ä»¶: {directory_path}")
        return False
    
    # åº”ç”¨æ–‡ä»¶æ•°é‡é™åˆ¶
    if max_files and max_files > 0:
        txt_files = txt_files[:max_files]
        print(f"ğŸ“š æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶ï¼ˆé™åˆ¶æœ€å¤š {max_files} ä¸ªï¼‰")
    else:
        print(f"ğŸ“š æ‰¾åˆ° {len(txt_files)} ä¸ªtxtæ–‡ä»¶")
    
    # æ¸…ç©ºå¤±è´¥æ–‡ä»¶è®°å½•
    global FAILED_FILES
    FAILED_FILES = {
        'encoding_errors': [],
        'content_errors': [],
        'database_errors': [],
        'duplicate_files': [],
        'other_errors': []
    }
    
    print("-" * 60)
    
    success_count = 0
    failed_count = 0
    
    for i, file_path in enumerate(txt_files, 1):
        print(f"[{i}/{len(txt_files)}] ", end="")
        if import_txt_file(file_path, category_id):
            success_count += 1
        else:
            failed_count += 1
        print()  # ç©ºè¡Œåˆ†éš”
    
    print("=" * 60)
    print(f"ğŸ“Š å¯¼å…¥å®Œæˆç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {success_count}")
    print(f"   âŒ å¤±è´¥: {failed_count}")
    print(f"   ğŸ“– æ€»è®¡: {len(txt_files)}")
    print(f"   ğŸ“‚ åˆ†ç±»: {category_info['name']} (ID: {category_id})")
    
    # ç”Ÿæˆå¤±è´¥æ–‡ä»¶æŠ¥å‘Š
    if failed_count > 0:
        report_file = save_failed_files_report()
        if report_file:
            print(f"\nğŸ’¡ ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤é‡è¯•å¤±è´¥çš„æ–‡ä»¶:")
            print(f"   python {sys.argv[0]} --retry {report_file} {category_id}")
            print(f"   python {sys.argv[0]} --retry {report_file} {category_id} --retry-type encoding")
    
    return success_count > 0

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="å°è¯´TXTå¯¼å…¥å·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  %(prog)s /path/to/txt/files 1                    # å¯¼å…¥ç›®å½•ä¸‹æ‰€æœ‰txtåˆ°åˆ†ç±»ID 1
  %(prog)s /path/to/txt/files 1 --max-files 10    # æœ€å¤šå¯¼å…¥10ä¸ªæ–‡ä»¶
  %(prog)s --list-categories                       # æŸ¥çœ‹æ‰€æœ‰å¯ç”¨åˆ†ç±»
  %(prog)s single.txt 2                           # å¯¼å…¥å•ä¸ªæ–‡ä»¶åˆ°åˆ†ç±»ID 2
  %(prog)s --retry report.json 1                  # é‡è¯•å¤±è´¥æ–‡ä»¶æŠ¥å‘Šä¸­çš„æ‰€æœ‰æ–‡ä»¶
  %(prog)s --retry report.json 1 --retry-type encoding  # åªé‡è¯•ç¼–ç é”™è¯¯çš„æ–‡ä»¶

æ³¨æ„äº‹é¡¹:
  - è¯·ç¡®ä¿æŒ‡å®šçš„åˆ†ç±»IDå­˜åœ¨
  - å·¥å…·ä¼šè‡ªåŠ¨è·³è¿‡å·²å­˜åœ¨çš„åŒåå°è¯´
  - æ”¯æŒå¤šç§æ–‡æœ¬ç¼–ç ï¼šUTF-8, GBK, GB2312, UTF-16, Big5ç­‰
  - å¤±è´¥çš„æ–‡ä»¶ä¼šç”Ÿæˆè¯¦ç»†çš„é”™è¯¯æŠ¥å‘Š
  - é‡è¯•ç±»å‹: encoding, content, database, duplicate, other
        """
    )
    
    parser.add_argument('path', nargs='?', help='txtæ–‡ä»¶æˆ–ç›®å½•è·¯å¾„')
    parser.add_argument('category_id', type=int, nargs='?', help='åˆ†ç±»ID')
    parser.add_argument('--max-files', type=int, help='æœ€å¤§å¯¼å…¥æ–‡ä»¶æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰')
    parser.add_argument('--list-categories', action='store_true', help='æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨åˆ†ç±»')
    parser.add_argument('--author', default='æœªçŸ¥ä½œè€…', help='é»˜è®¤ä½œè€…åç§°')
    parser.add_argument('--retry', help='é‡è¯•å¤±è´¥æ–‡ä»¶æŠ¥å‘Šçš„è·¯å¾„')
    parser.add_argument('--retry-type', choices=['encoding', 'content', 'database', 'duplicate', 'other'], 
                       help='æŒ‡å®šé‡è¯•çš„é”™è¯¯ç±»å‹')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("        å°è¯´TXTå¯¼å…¥å·¥å…· (å¢å¼ºç‰ˆ)")
    print("=" * 60)
    
    # æ˜¾ç¤ºåˆ†ç±»åˆ—è¡¨
    if args.list_categories:
        list_available_categories()
        return
    
    # é‡è¯•å¤±è´¥æ–‡ä»¶
    if args.retry:
        if args.category_id is None:
            print("âŒ é”™è¯¯: é‡è¯•æ—¶éœ€è¦æŒ‡å®šåˆ†ç±»ID")
            print("\nä½¿ç”¨ç¤ºä¾‹: python import_direct.py --retry report.json 1")
            return
        
        print(f"ğŸ”„ é‡è¯•å¤±è´¥æ–‡ä»¶æŠ¥å‘Š: {args.retry}")
        if args.retry_type:
            print(f"   é‡è¯•ç±»å‹: {args.retry_type}")
        else:
            print("   é‡è¯•æ‰€æœ‰å¤±è´¥æ–‡ä»¶ï¼ˆé™¤é‡å¤æ–‡ä»¶ï¼‰")
        print("-" * 60)
        
        if retry_failed_files(args.retry, args.category_id, args.retry_type):
            print("\nâœ… é‡è¯•å®Œæˆ!")
        else:
            print("\nâŒ é‡è¯•å¤±è´¥!")
        return
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.path or args.category_id is None:
        print("âŒ é”™è¯¯: éœ€è¦æŒ‡å®šè·¯å¾„å’Œåˆ†ç±»ID")
        print("\nä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯")
        print("ä½¿ç”¨ --list-categories æŸ¥çœ‹å¯ç”¨åˆ†ç±»")
        print("ä½¿ç”¨ --retry <report.json> <category_id> é‡è¯•å¤±è´¥æ–‡ä»¶")
        return
    
    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.path):
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {args.path}")
        return
    
    # åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
    if os.path.isfile(args.path):
        # å•ä¸ªæ–‡ä»¶å¯¼å…¥
        if not args.path.lower().endswith('.txt'):
            print(f"âŒ ä¸æ˜¯txtæ–‡ä»¶: {args.path}")
            return
        
        # éªŒè¯åˆ†ç±»
        category_exists, category_info = verify_category_exists(args.category_id)
        if not category_exists:
            print(f"âŒ åˆ†ç±»ID {args.category_id} ä¸å­˜åœ¨")
            list_available_categories()
            return
        
        print(f"ğŸ“– å¯¼å…¥å•ä¸ªæ–‡ä»¶: {args.path}")
        print(f"ğŸ“‚ ç›®æ ‡åˆ†ç±»: {category_info['name']} (ID: {args.category_id})")
        print("-" * 60)
        
        # æ¸…ç©ºå¤±è´¥æ–‡ä»¶è®°å½•
        global FAILED_FILES
        FAILED_FILES = {
            'encoding_errors': [],
            'content_errors': [],
            'database_errors': [],
            'duplicate_files': [],
            'other_errors': []
        }
        
        if import_txt_file(args.path, args.category_id, args.author):
            print("\nâœ… å•ä¸ªæ–‡ä»¶å¯¼å…¥æˆåŠŸ!")
        else:
            print("\nâŒ å•ä¸ªæ–‡ä»¶å¯¼å…¥å¤±è´¥!")
            # ç”Ÿæˆå¤±è´¥æŠ¥å‘Š
            report_file = save_failed_files_report()
            if report_file:
                print(f"ğŸ“Š å¤±è´¥æ–‡ä»¶æŠ¥å‘Š: {report_file}")
                print(f"ğŸ’¡ é‡è¯•å‘½ä»¤: python {sys.argv[0]} --retry {report_file} {args.category_id}")
            
    elif os.path.isdir(args.path):
        # ç›®å½•æ‰¹é‡å¯¼å…¥
        if batch_import_directory(args.path, args.category_id, args.max_files):
            print("\nâœ… æ‰¹é‡å¯¼å…¥å®Œæˆ!")
        else:
            print("\nâŒ æ‰¹é‡å¯¼å…¥å¤±è´¥!")
    else:
        print(f"âŒ æ— æ•ˆçš„è·¯å¾„ç±»å‹: {args.path}")

if __name__ == "__main__":
    main()
